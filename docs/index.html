<!DOCTYPE html>
<meta charset="utf-8">

<html>

<style type="text/css">
body {
	font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	font-weight: 300;
	font-size: 17px;
	margin-left: auto;
	margin-right: auto;
	width: 980px;
}
h1 {
	font-weight:300;
	line-height: 1.15em;
}

h2 {
	font-size: 1.75em;
}
a:link,a:visited {
	color: #1367a7;
	text-decoration: none;
}
a:hover {
	color: #208799;
}
h1, h2, h3 {
	text-align: center;
}
h1 {
	font-size: 40px;
	font-weight: 500;
}
h2, h3 {
	font-weight: 400;
	margin: 16px 0px 4px 0px;
}
.paper-title {
	padding: 16px 0px 16px 0px;
}
section {
	margin: 32px 0px 32px 0px;
	text-align: justify;
	clear: both;
}
.col-6 {
	width: 16.6%;
	float: left;
}
.col-5 {
	width: 20%;
	float: left;
}
.col-4 {
	width: 25%;
	float: left;
}

.col-3a {
	width: 33.3%;
	float: left;
	text-align: right;
}

.col-3b {
	width: 33.3%;
	float: left;
	text-align: center;
}

.col-3c {
	width: 33.3%;
	float: left;
	text-align: left;
}

.col-2 {
	width: 50%;
	float: left;
}
.row, .author-row, .affil-row {
	overflow: auto;
}
.author-row, .affil-row {
	font-size: 20px;
}
.row {
	margin: 16px 0px 16px 0px;
}
.authors {
	font-size: 18px;
}
.affil-row {
	margin-top: 16px;
}
.teaser {
	max-width: 100%;
}
.text-center {
	text-align: center;
}
.screenshot {
	width: 256px;
	border: 1px solid #ddd;
}
.screenshot-el {
	margin-bottom: 16px;
}
hr {
	height: 1px;
	border: 0;
	border-top: 1px solid #ddd;
	margin: 0;
}
.material-icons {
	vertical-align: -6px;
}
p {
	line-height: 1.25em;
}
.caption_justify {
	font-size: 16px;
	color: #666;
	text-align: justify;
	margin-top: 0px;
	margin-bottom: 64px;
}
.caption {
	font-size: 16px;
	color: #666;
	text-align: center;
	margin-top: 8px;
	margin-bottom: 64px;
}
.caption_inline {
	font-size: 16px;
	color: #666;
	text-align: center;
	margin-top: 8px;
	margin-bottom: 0px;
}
.caption_bold {
	font-size: 16px;
	color: #666;
	text-align: center;
	margin-top: 0px;
	margin-bottom: 0px;
	font-weight: bold;
}
video {
	display: block;
	margin: auto;
}
figure {
	display: block;
	margin: auto;
	margin-top: 10px;
	margin-bottom: 10px;
}
#bibtex pre {
	font-size: 14px;
	background-color: #eee;
	padding: 16px;
}
.blue {
	color: #2c82c9;
	font-weight: bold;
}
.orange {
	color: #d35400;
	font-weight: bold;
}
.flex-row {
	display: flex;
	flex-flow: row wrap;
	justify-content: space-around;
	padding: 0;
	margin: 0;
	list-style: none;
}
.paper-btn {
    position: relative;
    text-align: center;

    display: inline-block;
    margin: 8px;
    padding: 8px 8px;

    border-width: 0;
    outline: none;
    border-radius: 2px;

    background-color: #1367a7;
    color: #ecf0f1 !important;
    font-size: 20px;
    width: 100px;
    font-weight: 600;
}
.paper-btn-parent {
	display: flex;
	justify-content: center;
	margin: 16px 0px;
}
.paper-btn:hover {
	opacity: 0.85;
}
.container {
	margin-left: auto;
	margin-right: auto;
	padding-left: 16px;
	padding-right: 16px;
}

</style>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
	<title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title>
	<meta property="og:description" content="AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation"/>
	<link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
</head>

<body>
<div class="container">
	<div class="paper-title">
		<h1>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</h1>
	</div>

	<div id="authors">
		<div class="author-row">
			<div class="col-3a text-center"><a href="https://tkhkaeio.github.io/">Takehiko Ohkawa<sup>1,2</sup></a></div>
			<div class="col-3b text-center"><a href="https://scholar.google.com/citations?user=eLFhiSYAAAAJ&hl=en">Kun He<sup>1</sup></a></div>            
			<div class="col-3c text-center"><a href="https://scholar.google.com/citations?user=-juoweoAAAAJ">Fadime Sener<sup>1</sup></a></div>
			<div class="col-3a text-center"><a href="https://cmp.felk.cvut.cz/~hodanto2/">Tomáš Hodaň<sup>1</sup></a></div>
			<div class="col-3b text-center"><a href="https://scholar.google.co.jp/citations?user=YV0TVrQAAAAJ&hl">Luan Tran<sup>1</sup></a></div>
			<div class="col-3c text-center"><a href="https://scholar.google.com/citations?user=9HoiYnYAAAAJ&hl=en">Cem Keskin<sup>1</sup></a></div>
		</div>

		<div class="affil-row">
            <div class="col-1 text-center"><sup>1</sup>Meta Reality Labs&emsp;&emsp;<sup>2</sup>The University of Tokyo</div>
		</div>

		<div class="affil-row">
			<div class="text-center"><strong style="color: darkblue;">CVPR 2023</strong></div>
		</div>

        <!-- <div style="clear: both">
			<div class="paper-btn-parent">
				<a class="paper-btn" href="https://openaccess.thecvf.com/tbd">
					<span class="material-icons"> description </span>
					Paper
				</a>
				<a class="paper-btn" href="https://github.com/assemblyhands/tbd">
					<span class="material-icons"> file_download </span>
					Dataset
				</a>
				<a class="paper-btn" href="https://github.com/assemblyhands?tab=repositories">
					<span class="material-icons"> code </span>
					Code
				</a>
			</div>
		</div> -->
    </div>   
    
    <section id="teaser-videos">   
        <div class="row">
            <div class="col-xs-12 text-center">
                <iframe width="800" height="400" src="https://www.youtube.com/embed/34wmdm7_X8k?autoplay=1&mute=1&showinfo=0&modestbranding=1&controls=0&disablekb=1&playsinline=1&loop=1&playlist=34wmdm7_X8k" frameborder="0"></iframe>
            </div>
            <div class="col-xs-12 text-center" style="font-style: italic;">
                <b>Visualization of automatically generated annotations of 3D hand poses.</b><br>
                (Row 1-4: exocentric views, Row 5: egocentric views)
            </div>
        </div>
    </section>

    <section id="news">
		<h2>News</h2>
		<hr>
		<div class="row">			
			<div><span class="material-icons"> description </span> [April 2nd 2023] Project page released.</div>
		</div>
	</section>

    <section id="abstract">
        <h2>Abstract</h2>
        <hr>
        <p>
            We present AssemblyHands, a large-scale benchmark dataset with accurate 3D hand pose annotations, to facilitate the study of egocentric activities with challenging hand-object interactions. The dataset includes synchronized egocentric and exocentric images sampled from the recent Assembly101 dataset, in which participants assemble and disassemble take-apart toys. To obtain high-quality 3D hand pose annotations for the egocentric images,  we develop an efficient pipeline, where we use an initial set of manual annotations to train a model to automatically annotate a much larger dataset. Our annotation model uses multi-view feature fusion and an iterative refinement scheme, and achieves an average keypoint error of 4.20 mm, which is 85 % lower than the error of the original annotations in Assembly101. AssemblyHands provides 3.0M annotated images, including 490K egocentric images, making it the largest existing benchmark dataset for egocentric 3D hand pose estimation. Using this data, we develop a strong single-view baseline of 3D hand pose estimation from egocentric images. Furthermore, we design a novel action classification task to evaluate predicted 3D hand poses. Our study shows that having higher-quality hand poses directly improves the ability to recognize actions.
        </p>
        <div class="col-xs-12 text-center">
            <img src="https://user-images.githubusercontent.com/28190044/229331527-6da02520-af0a-4b75-a108-856bc47492cb.png" width="600">
        </div>
        <div class="col-xs-12" style="font-style: italic;margin: auto;">
            <b>High-quality 3D hand poses as an effective representation for egocentric activity understanding. </b>
            AssemblyHands provides high-quality 3D hand pose annotations computed from multi-view exocentric images sampled from Assembly101, which originally comes with inaccurate annotations computed from egocentric images (see the incorrect left-hand pose prediction). As we experimentally demonstrate on an action classification task, models trained on high-quality annotations achieve significantly higher accuracy.
        </div>
    </section>

    <section id="dataset">
        <h2>Dataset</h2>
        <hr>
        <div class="col-xs-12 text-center">
            <img src="https://user-images.githubusercontent.com/28190044/229331693-30f7fd3e-050e-4c38-98da-4dfba75fa053.png" width="960">
        </div>
        <div class="col-xs-12" style="font-style: italic;">
            <b>Comparison of AssemblyHands with existing 3D hand pose datasets.</b> “M” and “A” stand for manual and automatic annotation, respectively. AssemblyHands is the largest existing benchmark for egocentric 3D hand pose estimation.
        </div>
    </section>
    
		
	<section id="license">
		<h2>License</h2>
		<hr>
		<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />AssemblyHands is licensed by us under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>. The terms of this license are:
		<p><strong>Attribution</strong> : You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</p>
		<p><strong>NonCommercial</strong> : You may not use the material for commercial purposes.</p>        
	</section>
	

	<section id="bibtex">
		<h2>Citation</h2>
		<hr>
		<pre><code>@inproceedings{ohkawa:cvpr23,
    title     = {{AssemblyHands:} Towards Egocentric Activity Understanding via 3D Hand Pose Estimation},
    author    = {Takehiko Ohkawa and Kun He and Fadime Sener and Tomas Hodan and Luan Tran and Cem Keskin},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages     = {X--Y},
    year      = {2023},
}
    </code></pre>
</div>
</body>
</html>
